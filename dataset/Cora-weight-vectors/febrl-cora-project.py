# =============================================================================
# AUSTRALIAN NATIONAL UNIVERSITY OPEN SOURCE LICENSE (ANUOS LICENSE)
# VERSION 1.3
# 
# The contents of this file are subject to the ANUOS License Version 1.2
# (the "License"); you may not use this file except in compliance with
# the License. You may obtain a copy of the License at:
# 
#   http://datamining.anu.edu.au/linkage.html
# 
# Software distributed under the License is distributed on an "AS IS"
# basis, WITHOUT WARRANTY OF ANY KIND, either express or implied. See
# the License for the specific language governing rights and limitations
# under the License.
# 
# The Original Software is: "febrl-cora-project.py"
# 
# The Initial Developers of the Original Software are:
#   Peter Christen
# 
# Copyright (C) 2002 - 2011 the Australian National University and
# others. All Rights Reserved.
# 
# Contributors:
# 
# Alternatively, the contents of this file may be used under the terms
# of the GNU General Public License Version 2 or later (the "GPL"), in
# which case the provisions of the GPL are applicable instead of those
# above. The GPL is available at the following URL: http://www.gnu.org/
# If you wish to allow use of your version of this file only under the
# terms of the GPL, and not to allow others to use your version of this
# file under the terms of the ANUOS License, indicate your decision by
# deleting the provisions above and replace them with the notice and
# other provisions required by the GPL. If you do not delete the
# provisions above, a recipient may use your version of this file under
# the terms of any one of the ANUOS License or the GPL.
# =============================================================================

print '='*70
print 'Febrl classification on "Cora"'

# =============================================================================
# Start of Febrl project module: "febrl-cora-project.py"
#
# Generated using "guiFebrl.py" on Sat Jun 14 19:36:26 2014
# =============================================================================

import sys
sys.path.append('/home/christen/projects/febrl')  #  On adamms2

# Import necessary modules (Python standard modules first, then Febrl modules)

import logging
import time

import classification
import comparison
import dataset
import encode
import indexing
import measurements
import mymath
import output
import stringcmp

start_time = time.time()

# -----------------------------------------------------------------------------
# Intialise a logger, set level to info oe warning
#
log_level = logging.WARNING #INFO # logging.WARNING

my_logger = logging.getLogger()
my_logger.setLevel(log_level)

# -----------------------------------------------------------------------------
# Febrl project type: Deduplicate
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------

# Define input data set A:
#
data_set_a = dataset.DataSetCSV(description="Data set generated by Febrl GUI",
                                access_mode="read",
                                strip_fields=True,
                                miss_val=[''],
                                rec_ident="REC_ID",
                                file_name="../datasets/cora.csv",
                                header_line=False,
                                delimiter=",",
                                field_list = [("field-0",0),
                                              ("ID",1),
                                              ("AUTHORS",2),
                                              ("field-3",3),
                                              ("TITLE",4),
                                              ("field-5",5),
                                              ("VENUE",6),     # 10.5% missing
                                              ("ADDRESS",7),   # 77.5% missing
                                              ("PUBLISHER",8), # 69.9% missing
                                              ("YEAR",9),      # 12.3% missing
                                              ("PAGES",10),    # 32.9% missing
                                              ("field-11",11),
                                              ("field-12",12),
                                              ("field-13",13)])

# -----------------------------------------------------------------------------

# Define field comparison functions
#

# Exact match to get true match status
#
fc_funct_0 = comparison.FieldComparatorExactString(agree_weight = 1.0,
                               description = "Str-Exact-ID-ID",
                               disagree_weight = 0.0,
                               missing_weight = 0.0)


fc_funct_1 = comparison.FieldComparatorQGram(agree_weight = 1.0,
                               description = "Q-Gram-AUTHORS-AUTHORS",
                               disagree_weight = 0.0,
                               missing_weight = 0.0,
                               threshold = 0.0,
                               q = 3,
                               common_divisor = "average",
                               padded = True)

fc_funct_2 = comparison.FieldComparatorQGram(agree_weight = 1.0,
                               description = "Q-Gram-TITLE-TITLE",
                               disagree_weight = 0.0,
                               missing_weight = 0.0,
                               threshold = 0.0,
                               q = 3,
                               common_divisor = "average",
                               padded = True)

fc_funct_3 = comparison.FieldComparatorQGram(agree_weight = 1.0,
                               description = "Q-Gram-VENUE-VENUE",
                               disagree_weight = 0.0,
                               missing_weight = 0.0,
                               threshold = 0.0,
                               q = 3,
                               common_divisor = "average",
                               padded = True)

fc_funct_4 = comparison.FieldComparatorQGram(agree_weight = 1.0,
                               description = "Q-Gram-PUBLISHER-PUBLISHER",
                               disagree_weight = 0.0,
                               missing_weight = 0.0,
                               threshold = 0.0,
                               q = 2,
                               common_divisor = "average",
                               padded = True)

fc_funct_5 = comparison.FieldComparatorEditDist(agree_weight = 1.0,
                               description = "Edit-Dist-YEAR-YEAR",
                               disagree_weight = 0.0,
                               missing_weight = 0.0,
                               threshold = 0.0)

field_comp_list = [(fc_funct_0, "ID", "ID"),  # 1.0 if true match
                   (fc_funct_1, "AUTHORS", "AUTHORS"),
                   (fc_funct_2, "TITLE", "TITLE"),
                   (fc_funct_3, "VENUE", "VENUE"),
#                   (fc_funct_4, "PUBLISHER", "PUBLISHER"),
                   (fc_funct_5, "YEAR", "YEAR")
                  ]

rec_comp = comparison.RecordComparator(data_set_a, data_set_a, field_comp_list)

# -----------------------------------------------------------------------------

# Define indices for "blocking"
#
index_def_1 = [["YEAR", "YEAR", False, False, 4, []]]

index_def_2 = [["VENUE", "VENUE", False, False, 4, [encode.dmetaphone]]]

index_def_3 = [["PUBLISHER", "PUBLISHER", False, False, 4, [encode.dmetaphone]]]

index_def_4 = [["AUTHORS", "AUTHORS", False, False, 4, [encode.dmetaphone]]]

index_def_5 = [["TITLE", "TITLE", False, False, 4, [encode.dmetaphone]]]

# AUTHORS words sorted and reversed
#
index_def_6 = [["AUTHORS", "AUTHORS", True, True, 4, [encode.dmetaphone]]]

index = indexing.DedupIndex(dataset1 = data_set_a,
                            dataset2 = data_set_a,
                            progress_report = 10,
#                            weight_vec_file = "./cora-weight-vectors.csv",
                            rec_comparator = rec_comp,
                            index_sep_str = "",
                            skip_missing = True,
                            index_def = [index_def_1,
                                         index_def_2,
                                         index_def_3,
                                         index_def_4,
                                         index_def_5,
                                         index_def_6],
                            block_method = ("block",))

# Build and compact index
#
index.build()

index.compact()

# Do record pair comparisons
#
[field_names_list, w_vec_dict] = index.run()

print
print 'Pair-wise comparison time used:', time.time() - start_time

# -----------------------------------------------------------------------------
# Prepare weight vectors for classification

# Define function to get the match status
#
def match_status_funct(red_id1, rec_id2, w_vec):
  return (w_vec[0] == 1.0)

# Get true match and non-match sets
#
[tm_set, tnm_set] = classification.get_true_matches_nonmatches(w_vec_dict,
                                                            match_status_funct)

# Remove the match status element from weight vectors
#
class_w_vec_dict = classification.extract_collapse_weight_vectors([(1,2,3,4)],
                                                                  w_vec_dict)

# Optimal threshold classifier ================================================

print
print 'Optimal threshold classifier'
print '----------------------------'

start_time = time.time()

# Define weight vector (record pair) classifier
#
classifier = classification.OptimalThreshold(bin_width = 0.01,
                                             min_method = "pos-neg")

# Supervised training of classifier
#
classifier.train(class_w_vec_dict, tm_set, tnm_set)

# Test quality of trained classifier
#
[tp, fn, fp, tn] = classifier.test(class_w_vec_dict, tm_set, tnm_set)

class_time = time.time()- start_time
print '  Classification time:', class_time

print "  Number of true positives: ", tp
print "  Number of false positives:", fp
print "  Number of true negatives: ", tn
print "  Number of false negatives:", fn
print

# Classify all weight vectors
#
[m_set, nm_set, pm_set] = classifier.classify(class_w_vec_dict)
assert len(pm_set) == 0
assert len(m_set) ==  tp+fp
assert len(nm_set) == tn+fn

# Evaluate linkage quality and complexity
#
acc, prec, reca, fmeas = measurements.quality_measures(w_vec_dict, m_set,
                                                       nm_set,
                                                       match_status_funct)
print "  Accuracy: ", acc
print "  Precision:", prec
print "  Recall:   ", reca
print "  F-measure:", fmeas

rr = measurements.reduction_ratio(class_w_vec_dict, data_set_a, data_set_a)
assert rr >= 0.0
print "  Reduction ratio:", rr

pq = measurements.pairs_quality(class_w_vec_dict, match_status_funct)
print "  Pairs quality:  ", pq

# k-NN based two-step classifier ==============================================

print
print 'K-NN Two-step classifier'
print '------------------------'

start_time = time.time()

# Define weight vector (record pair) classifier
#
classifier = classification.TwoStep(s1_match_method = (1.0,    "nearest",11,
                                                       True),
                                    s1_non_match_method = (0.0,"nearest",111,
                                                           True),
                                    s2_classifier = ("kmeans",mymath.distL2,11))

# Supervised training of classifier
#
classifier.train(class_w_vec_dict, tm_set, tnm_set)

# Test quality of trained classifier
#
[tp, fn, fp, tn] = classifier.test(class_w_vec_dict, tm_set, tnm_set)

class_time = time.time()- start_time
print '  Classification time:', class_time

print "  Number of true positives: ", tp
print "  Number of false positives:", fp
print "  Number of true negatives: ", tn
print "  Number of false negatives:", fn
print

# Classify all weight vectors
#
[m_set, nm_set, pm_set] = classifier.classify(class_w_vec_dict)
assert len(pm_set) == 0
assert len(m_set) ==  tp+fp
assert len(nm_set) == tn+fn

# Evaluate linkage quality and complexity
#
acc, prec, reca, fmeas = measurements.quality_measures(w_vec_dict, m_set,
                                                       nm_set,
                                                       match_status_funct)
print "  Accuracy: ", acc
print "  Precision:", prec
print "  Recall:   ", reca
print "  F-measure:", fmeas

rr = measurements.reduction_ratio(class_w_vec_dict, data_set_a, data_set_a)
assert rr >= 0.0
print "  Reduction ratio:", rr

pq = measurements.pairs_quality(class_w_vec_dict, match_status_funct)
print "  Pairs quality:  ", pq

# K-means classifier ==========================================================

print
print 'K-means classifier'
print '------------------'

start_time = time.time()

classifier = classification.KMeans(dist_measure = mymath.distL2,
                                   max_iter_count = 10000,
                                   centroid_init = "min/max")

classifier.train(class_w_vec_dict, set(), set())

# Test quality of trained classifier
#
[tp, fn, fp, tn] = classifier.test(class_w_vec_dict, tm_set, tnm_set)

class_time = time.time()- start_time
print '  Classification time:', class_time

print "  Number of true positives: ", tp
print "  Number of false positives:", fp
print "  Number of true negatives: ", tn
print "  Number of false negatives:", fn
print

# Classify all weight vectors
#
[m_set, nm_set, pm_set] = classifier.classify(class_w_vec_dict)
assert len(pm_set) == 0
assert len(m_set) ==  tp+fp
assert len(nm_set) == tn+fn

# Evaluate linkage quality and complexity
#
acc, prec, reca, fmeas = measurements.quality_measures(w_vec_dict, m_set,
                                                       nm_set,
                                                       match_status_funct)
print "  Accuracy: ", acc
print "  Precision:", prec
print "  Recall:   ", reca
print "  F-measure:", fmeas

rr = measurements.reduction_ratio(class_w_vec_dict, data_set_a, data_set_a)
assert rr >= 0.0
print "  Reduction ratio:", rr

pq = measurements.pairs_quality(class_w_vec_dict, match_status_funct)
print "  Pairs quality:  ", pq

# Farthest first classifier ===================================================

print
print 'Farthest first classifier'
print '-------------------------'

start_time = time.time()

classifier = classification.FarthestFirst(dist_measure = mymath.distL2,
                                          centroid_init = "min/max")

classifier.train(class_w_vec_dict, set(), set())

# Test quality of trained classifier
#
[tp, fn, fp, tn] = classifier.test(class_w_vec_dict, tm_set, tnm_set)

class_time = time.time()- start_time
print '  Classification time:', class_time

print "  Number of true positives: ", tp
print "  Number of false positives:", fp
print "  Number of true negatives: ", tn
print "  Number of false negatives:", fn
print

# Classify all weight vectors
#
[m_set, nm_set, pm_set] = classifier.classify(class_w_vec_dict)
assert len(pm_set) == 0
assert len(m_set) ==  tp+fp
assert len(nm_set) == tn+fn

# Evaluate linkage quality and complexity
#
acc, prec, reca, fmeas = measurements.quality_measures(w_vec_dict, m_set,
                                                       nm_set,
                                                       match_status_funct)
print "  Accuracy: ", acc
print "  Precision:", prec
print "  Recall:   ", reca
print "  F-measure:", fmeas

rr = measurements.reduction_ratio(class_w_vec_dict, data_set_a, data_set_a)
assert rr >= 0.0
print "  Reduction ratio:", rr

pq = measurements.pairs_quality(class_w_vec_dict, match_status_funct)
print "  Pairs quality:  ", pq
print

# =============================================================================
# End of Febrl project module: "febrl-cora-project.py"
# =============================================================================
